{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ebf509c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, ConfusionMatrixDisplay\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bd4b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLYDataset(Dataset):\n",
    "    def __init__(self, path_to_data, mode = \"training\", cam = 0):\n",
    "\n",
    "        # save selected camera and init lists\n",
    "        self.cam = cam\n",
    "        self.img_paths = []\n",
    "        self.annotations = []\n",
    "        self.H = 480\n",
    "        self.W = 980\n",
    "\n",
    "        # e.g path to the data, different classes, number of images per class, or image IDs per class\n",
    "        if(mode != \"testing\" and mode != \"training\"):\n",
    "            raise ValueError(\"No such kind of data available\")\n",
    "\n",
    "        #Create path from path and parameter training/test\n",
    "        full_path = os.path.join(path_to_data, mode, f\"cam{cam}\")\n",
    "        # check if path exists. Path to data and kind are user inputs.\n",
    "        if not os.path.isdir(full_path):\n",
    "            raise FileExistsError(f\"Wrong path {full_path}\")\n",
    "\n",
    "        # get the path to the annotation file and the path to the images\n",
    "        annotation_file_path = os.path.join(full_path, \"annotations\", \"annotations.npz\")\n",
    "        image_path = os.path.join(full_path, \"images\")\n",
    "        # check if path exists. \n",
    "        if not os.path.isfile(annotation_file_path):\n",
    "            raise FileExistsError(f\"Wrong path to annotation file {annotation_file_path}\")\n",
    "        if not os.path.isdir(image_path):\n",
    "            raise FileExistsError(f\"Wrong path {image_path}\")\n",
    "        \n",
    "        # load annotations\n",
    "        annotations = np.load(annotation_file_path)\n",
    "        self.annotations.append(annotations['points2d'])\n",
    "\n",
    "        for image_name in sorted(os.listdir(image_path)):\n",
    "                # get the label as an int and the path of the image\n",
    "                if(image_name.endswith(\".jpg\")):\n",
    "                    self.img_paths.append(os.path.join(image_path, image_name))\n",
    "\n",
    "        if len(self.img_paths) != len(self.annotations):\n",
    "            raise IndexError(\"Number of images and annotations must be the same\")\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returning a single image per given index idx and its corresponding label\n",
    "\n",
    "        #Check if the index exists and read in the image. Return both the image and the label as torch tensors\n",
    "        if(idx >= len(self.img_paths)):\n",
    "            raise LookupError(\"Invalid index for image\")\n",
    "        img = cv2.imread(self.img_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        #creating torch tensor and normalizing,a lso adding front channel to match requirement from torch\n",
    "        t_img = torch.tensor(img, dtype=torch.float32) / 255\n",
    "        t_img = t_img.unsqueeze(0)\n",
    "\n",
    "        # prepare annotations as tensor\n",
    "        annotation = self.annotations[idx]\n",
    "        t_anno = torch.tensor(annotation, dtype=torch.float32)\n",
    "\n",
    "        return t_img, t_anno\n",
    "    \n",
    "    def __len__(self):\n",
    "        # returning whole length of the dataset / number of images\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getvisual__(self, idx = 0):\n",
    "        if(idx >= len(self.img_paths)):\n",
    "            raise LookupError(\"Invalid index for image\")\n",
    "        img = cv2.imread(self.img_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        annotation = self.annotations[idx]\n",
    "        annotation[0,:] *= self.W\n",
    "        annotation[0,:] *= self.H\n",
    "        \n",
    "        return img, annotation\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e9117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = FLYDataset(\"/home/maurrow/Documents/Uni_Projects/uzk_cvproject/data\")\n",
    "plt.axis('off')\n",
    "x, y = dataset.__getvisual__()\n",
    "print(y.shape)\n",
    "plt.imshow(x, cmap=\"grey\")\n",
    "plt.scatter(y[:,0], y[:,1])\n",
    "\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7dd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Fly(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        \"\"\"Encoder\"\"\"\n",
    "        self.e1 = nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3), padding=1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.e2 = nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1)\n",
    "        self.e3 = nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1)\n",
    "        self.e4 = nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1)\n",
    "\n",
    "        \"\"\"Bottleneck\"\"\"\n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.linear = nn.Linear(3136, 2*embedding_size)\n",
    "\n",
    "        \"\"\"Decoder\"\"\"\n",
    "        self.d1 = torch.nn.Linear(embedding_size, 3136)\n",
    "        self.reshape = Reshape(-1, 64, 7, 7)\n",
    "        self.d2 = nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1)\n",
    "        self.d3 = nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1)\n",
    "        self.d4 = nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0)\n",
    "        self.d5 = nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0)\n",
    "        self.trim = Trim(input_size, input_size)\n",
    "        self.last_activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def encoder(self, inputs):\n",
    "        x = self.e1(inputs)\n",
    "        x = self.activation(x)\n",
    "        x = self.e2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.e3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.e4(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    def decoder(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.d2(self.reshape(x))\n",
    "        x = self.activation(x)\n",
    "        x = self.d3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.d4(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.d5(x)\n",
    "\n",
    "        \"\"\"Output\"\"\"\n",
    "        outputs = self.last_activation(self.trim(x))\n",
    "        return outputs\n",
    "    \n",
    "    # apply std and epsilon to make data more noisy\n",
    "    def reparametrization(self, mu, logsig):\n",
    "        std = torch.exp(logsig * 0.5)\n",
    "        noise = torch.randn_like(std)\n",
    "        return mu + (std * noise)\n",
    "    \n",
    "    # calculate loss by combining mse and kld\n",
    "    @staticmethod\n",
    "    def loss_function(pred_x, inputs, pred_mean, pred_logvar, kl_weight=1):\n",
    "        rec_loss = F.mse_loss(pred_x, inputs)\n",
    "        kld_loss = -0.5 * torch.mean(1 + pred_logvar - pred_mean.pow(2) - pred_logvar.exp())\n",
    "        loss = rec_loss + kl_weight * kld_loss\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss': rec_loss.detach(), 'KLD': -kld_loss.detach() }\n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor) of generated samples (shape num_samples, input_size)\n",
    "        \"\"\"\n",
    "    def generate_from_latent(self, num_samples:int, current_device: int, **kwargs):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # create random samples from std\n",
    "            samples = torch.randn(num_samples, self.embedding_size).to(current_device)\n",
    "            # decode\n",
    "            generated = self.decoder(samples)\n",
    "            return generated\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Encoder\"\"\"\n",
    "        x = self.encoder(inputs)\n",
    "\n",
    "        \"\"\"Bottleneck\"\"\"\n",
    "        x = self.linear(self.flatten(x))\n",
    "\n",
    "        # the mu logvar split as defined in the task\n",
    "        mu, logvar = torch.chunk(x, 2, dim=1)\n",
    "        noisy = self.reparametrization(mu, logvar)\n",
    "\n",
    "        \"\"\"Decoder\"\"\"\n",
    "        outputs = self.decoder(noisy)\n",
    "\n",
    "        return outputs, noisy, mu, logvar\n",
    "\n",
    "    def get_embedding(self, inputs):\n",
    "        \"\"\"Encoder\"\"\"\n",
    "        x = self.encoder(inputs)\n",
    "\n",
    "        \"\"\"Bottleneck\"\"\"\n",
    "        x = self.linear(self.flatten(x))\n",
    "\n",
    "        # the mu logvar split as defined in the task\n",
    "        mu, logvar = torch.chunk(x, 2, dim=1)\n",
    "        noisy = self.reparametrization(mu, logvar)\n",
    "\n",
    "        return noisy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "kl_weight = 0.01\n",
    "learning_rate = 1e-2\n",
    "epochs = 101\n",
    "\n",
    "vae = CNN_VAE(input_size, embedding_size).to(device)\n",
    "optimizer = optim.SGD(vae.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "print(f'\\nVAE -- With LR = {learning_rate} AND KL_WEIGHT = {kl_weight}')\n",
    "\n",
    "vae.zero_grad()\n",
    "history_loss = []\n",
    "history_val_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_recons_loss = 0.0\n",
    "    running_kld_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, _ = data[0].to(device), data[1].to(device)\n",
    "        if inputs.dim() == 3:\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        pred_x, _, pred_mean, pred_logvar = vae(inputs)\n",
    "        loss = vae.loss_function(pred_x, inputs, pred_mean, pred_logvar, kl_weight=kl_weight)\n",
    "        loss['loss'].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss['loss'].item()\n",
    "        running_recons_loss += loss['Reconstruction_Loss'].item()\n",
    "        running_kld_loss += loss['KLD'].item()\n",
    "    # output loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            running_val_loss = 0.0\n",
    "            running_val_recons_loss = 0.0\n",
    "            running_val_kld_loss = 0.0\n",
    "            for j, data in enumerate(test_dataloader, 0):\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                if inputs.dim() == 3:\n",
    "                    inputs = inputs.unsqueeze(1)\n",
    "                pred_x, _, pred_mean, pred_logvar = vae(inputs)\n",
    "                loss = vae.loss_function(pred_x, inputs, pred_mean, pred_logvar, kl_weight=kl_weight)\n",
    "\n",
    "                running_val_loss += loss['loss'].item()\n",
    "                running_val_recons_loss += loss['Reconstruction_Loss'].item()\n",
    "                running_val_kld_loss += loss['KLD'].item()\n",
    "\n",
    "            print(f'[{epoch + 1}] Loss: {running_loss / (i + 1):.5f} - Recons loss: {running_recons_loss / (i + 1):.5f}  '\n",
    "                 f'- KLD: {running_kld_loss / (i + 1):.5f} || Val loss: {running_val_loss / (j + 1):.5f} '\n",
    "                 f'- Recons val loss: {running_val_recons_loss / (j + 1):.5f}  - KLD: {running_val_kld_loss / (j + 1):.5f}')\n",
    "\n",
    "        history_loss.append(running_loss / (i + 1))\n",
    "        history_val_loss.append(running_val_loss / (j + 1))\n",
    "        running_loss = 0.0\n",
    "\n",
    "    # plot every n-th epoch\n",
    "    if epoch % plot_nepoch == 0:\n",
    "        rd_i = np.random.randint(inputs.shape[0])\n",
    "        fig, axes = plt.subplots(1, 2)\n",
    "        fig.suptitle(f'VAE - epoch {epoch} - loss {running_val_loss / (j + 1):.5f}')\n",
    "        axes[0].imshow(inputs[rd_i].reshape((28, 28)).detach().cpu().numpy())\n",
    "        axes[1].imshow(pred_x[rd_i].reshape((28, 28)).detach().cpu().numpy())\n",
    "        plt.savefig(os.path.join(path_, f'cnnvae_ex_epoch{epoch}_emb-{embedding_size}.png'))\n",
    "\n",
    "print('-- End training, saving model')\n",
    "torch.save(vae.state_dict(), os.path.join(path_, f'mnist_cnnvae_emb-{embedding_size}_klweight-{kl_weight}.pt'))\n",
    "\n",
    "# visualization\n",
    "plt.figure()\n",
    "plt.suptitle('Losses VAE')\n",
    "plt.plot(np.arange(1, len(history_loss) * 10 + 1, 10), history_loss, label='loss')\n",
    "plt.plot(np.arange(1, len(history_loss) * 10 + 1, 10), history_val_loss, '--', label='val_loss')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(path_, f'losses_emb-{embedding_size}_klweight-{kl_weight}.png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "df3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
