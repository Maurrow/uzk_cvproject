{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ebf509c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a23bbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd4b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "lass FLYDataset(Dataset):\n",
    "    def __init__(self, path_to_data, mode = \"training\"):\n",
    "        # e.g path to the data, different classes, number of images per class, or image IDs per class\n",
    "        if(mode != \"testing\" and mode != \"training\"):\n",
    "            raise ValueError(\"No such kind of data available\")\n",
    "\n",
    "        #Create path from path and parameter training/test\n",
    "        full_path = os.path.join(path_to_data, mode)\n",
    "        # check if path exists. Path to data and kind are user inputs.\n",
    "        if not os.path.isdir(full_path):\n",
    "            raise FileExistsError(f\"Wrong full path {full_path}\")\n",
    "\n",
    "        # init lists\n",
    "        self.img_labels = []\n",
    "        self.img_annots = []\n",
    "\n",
    "        # iterare over each label folder, append the label and the corresponding path to the data under the same index.\n",
    "        for label in sorted(os.listdir(full_path)):\n",
    "            label_path = os.path.join(full_path, label)\n",
    "            for image_name in os.listdir(label_path):\n",
    "                # get the label as an int and the path of the image\n",
    "                self.img_labels.append(int(label))\n",
    "                self.img_snnotes.append(os.path.join(label_path, image_name))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # insert code here\n",
    "        # returning a single image per given index idx and its corresponding label\n",
    "\n",
    "        #Check if the index exists and read in the image. Return both the image and the label\n",
    "        if(idx > len(self.img_labels)):\n",
    "            raise LookupError(\"Invalid index for image\")\n",
    "        img = mpimg.imread(self.img_data[idx])\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        # insert code here\n",
    "        # returning whole length of the dataset / number of images\n",
    "        return len(self.img_labels)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7dd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_VAE(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        \"\"\"Encoder\"\"\"\n",
    "        self.e1 = nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3), padding=1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.e2 = nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1)\n",
    "        self.e3 = nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1)\n",
    "        self.e4 = nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1)\n",
    "\n",
    "        \"\"\"Bottleneck\"\"\"\n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.linear = nn.Linear(3136, 2*embedding_size)\n",
    "\n",
    "        \"\"\"Decoder\"\"\"\n",
    "        self.d1 = torch.nn.Linear(embedding_size, 3136)\n",
    "        self.reshape = Reshape(-1, 64, 7, 7)\n",
    "        self.d2 = nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1)\n",
    "        self.d3 = nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1)\n",
    "        self.d4 = nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0)\n",
    "        self.d5 = nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0)\n",
    "        self.trim = Trim(input_size, input_size)\n",
    "        self.last_activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def encoder(self, inputs):\n",
    "        x = self.e1(inputs)\n",
    "        x = self.activation(x)\n",
    "        x = self.e2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.e3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.e4(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    def decoder(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.d2(self.reshape(x))\n",
    "        x = self.activation(x)\n",
    "        x = self.d3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.d4(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.d5(x)\n",
    "\n",
    "        \"\"\"Output\"\"\"\n",
    "        outputs = self.last_activation(self.trim(x))\n",
    "        return outputs\n",
    "    \n",
    "    # apply std and epsilon to make data more noisy\n",
    "    def reparametrization(self, mu, logsig):\n",
    "        std = torch.exp(logsig * 0.5)\n",
    "        noise = torch.randn_like(std)\n",
    "        return mu + (std * noise)\n",
    "    \n",
    "    # calculate loss by combining mse and kld\n",
    "    @staticmethod\n",
    "    def loss_function(pred_x, inputs, pred_mean, pred_logvar, kl_weight=1):\n",
    "        rec_loss = F.mse_loss(pred_x, inputs)\n",
    "        kld_loss = -0.5 * torch.mean(1 + pred_logvar - pred_mean.pow(2) - pred_logvar.exp())\n",
    "        loss = rec_loss + kl_weight * kld_loss\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss': rec_loss.detach(), 'KLD': -kld_loss.detach() }\n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor) of generated samples (shape num_samples, input_size)\n",
    "        \"\"\"\n",
    "    def generate_from_latent(self, num_samples:int, current_device: int, **kwargs):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # create random samples from std\n",
    "            samples = torch.randn(num_samples, self.embedding_size).to(current_device)\n",
    "            # decode\n",
    "            generated = self.decoder(samples)\n",
    "            return generated\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Encoder\"\"\"\n",
    "        x = self.encoder(inputs)\n",
    "\n",
    "        \"\"\"Bottleneck\"\"\"\n",
    "        x = self.linear(self.flatten(x))\n",
    "\n",
    "        # the mu logvar split as defined in the task\n",
    "        mu, logvar = torch.chunk(x, 2, dim=1)\n",
    "        noisy = self.reparametrization(mu, logvar)\n",
    "\n",
    "        \"\"\"Decoder\"\"\"\n",
    "        outputs = self.decoder(noisy)\n",
    "\n",
    "        return outputs, noisy, mu, logvar\n",
    "\n",
    "    def get_embedding(self, inputs):\n",
    "        \"\"\"Encoder\"\"\"\n",
    "        x = self.encoder(inputs)\n",
    "\n",
    "        \"\"\"Bottleneck\"\"\"\n",
    "        x = self.linear(self.flatten(x))\n",
    "\n",
    "        # the mu logvar split as defined in the task\n",
    "        mu, logvar = torch.chunk(x, 2, dim=1)\n",
    "        noisy = self.reparametrization(mu, logvar)\n",
    "\n",
    "        return noisy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "kl_weight = 0.01\n",
    "learning_rate = 1e-2\n",
    "epochs = 101\n",
    "\n",
    "vae = CNN_VAE(input_size, embedding_size).to(device)\n",
    "optimizer = optim.SGD(vae.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "print(f'\\nVAE -- With LR = {learning_rate} AND KL_WEIGHT = {kl_weight}')\n",
    "\n",
    "vae.zero_grad()\n",
    "history_loss = []\n",
    "history_val_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_recons_loss = 0.0\n",
    "    running_kld_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, _ = data[0].to(device), data[1].to(device)\n",
    "        if inputs.dim() == 3:\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        pred_x, _, pred_mean, pred_logvar = vae(inputs)\n",
    "        loss = vae.loss_function(pred_x, inputs, pred_mean, pred_logvar, kl_weight=kl_weight)\n",
    "        loss['loss'].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss['loss'].item()\n",
    "        running_recons_loss += loss['Reconstruction_Loss'].item()\n",
    "        running_kld_loss += loss['KLD'].item()\n",
    "    # output loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            running_val_loss = 0.0\n",
    "            running_val_recons_loss = 0.0\n",
    "            running_val_kld_loss = 0.0\n",
    "            for j, data in enumerate(test_dataloader, 0):\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                if inputs.dim() == 3:\n",
    "                    inputs = inputs.unsqueeze(1)\n",
    "                pred_x, _, pred_mean, pred_logvar = vae(inputs)\n",
    "                loss = vae.loss_function(pred_x, inputs, pred_mean, pred_logvar, kl_weight=kl_weight)\n",
    "\n",
    "                running_val_loss += loss['loss'].item()\n",
    "                running_val_recons_loss += loss['Reconstruction_Loss'].item()\n",
    "                running_val_kld_loss += loss['KLD'].item()\n",
    "\n",
    "            print(f'[{epoch + 1}] Loss: {running_loss / (i + 1):.5f} - Recons loss: {running_recons_loss / (i + 1):.5f}  '\n",
    "                 f'- KLD: {running_kld_loss / (i + 1):.5f} || Val loss: {running_val_loss / (j + 1):.5f} '\n",
    "                 f'- Recons val loss: {running_val_recons_loss / (j + 1):.5f}  - KLD: {running_val_kld_loss / (j + 1):.5f}')\n",
    "\n",
    "        history_loss.append(running_loss / (i + 1))\n",
    "        history_val_loss.append(running_val_loss / (j + 1))\n",
    "        running_loss = 0.0\n",
    "\n",
    "    # plot every n-th epoch\n",
    "    if epoch % plot_nepoch == 0:\n",
    "        rd_i = np.random.randint(inputs.shape[0])\n",
    "        fig, axes = plt.subplots(1, 2)\n",
    "        fig.suptitle(f'VAE - epoch {epoch} - loss {running_val_loss / (j + 1):.5f}')\n",
    "        axes[0].imshow(inputs[rd_i].reshape((28, 28)).detach().cpu().numpy())\n",
    "        axes[1].imshow(pred_x[rd_i].reshape((28, 28)).detach().cpu().numpy())\n",
    "        plt.savefig(os.path.join(path_, f'cnnvae_ex_epoch{epoch}_emb-{embedding_size}.png'))\n",
    "\n",
    "print('-- End training, saving model')\n",
    "torch.save(vae.state_dict(), os.path.join(path_, f'mnist_cnnvae_emb-{embedding_size}_klweight-{kl_weight}.pt'))\n",
    "\n",
    "# visualization\n",
    "plt.figure()\n",
    "plt.suptitle('Losses VAE')\n",
    "plt.plot(np.arange(1, len(history_loss) * 10 + 1, 10), history_loss, label='loss')\n",
    "plt.plot(np.arange(1, len(history_loss) * 10 + 1, 10), history_val_loss, '--', label='val_loss')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(path_, f'losses_emb-{embedding_size}_klweight-{kl_weight}.png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "df3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
