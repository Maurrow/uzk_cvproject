{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torchvision.transforms.functional as F\n",
    "from fly_dataset import FLYDataset\n",
    "from fly_cnn_old import CNN_Fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Keypoints(nn.Module):\n",
    "    def __init__(self, num_joints=38):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "        # Replace final classification layer with regression head\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_joints * 2)\n",
    "        )\n",
    "\n",
    "        self.num_joints = num_joints\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)               # shape: [B, num_joints * 2]\n",
    "        out = out.view(-1, self.num_joints, 2)  # shape: [B, J, 2]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Device: cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 1, 3, 3], expected input[1, 3, 480, 960] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scratch/cv-course2025/group2/uzk_cvproject/fly-test-resnet50.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 66\u001b[0m \u001b[43mvisualize_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m, in \u001b[0;36mvisualize_predictions\u001b[0;34m(model, dataset, device, num_samples)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Run model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Model returns a batch, meaning [B, J, 2], with squeezing 0 we \u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# end up with [J, 2].\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     pred_kp, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu() \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Get H, W from the visualize image shape. We permuted earlier, \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# meaning now we just cut of the last dim to end up with [H, W].\u001b[39;00m\n\u001b[1;32m     26\u001b[0m H, W \u001b[38;5;241m=\u001b[39m vis_img\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/assignments/ae-vae/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/assignments/ae-vae/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/cv-course2025/group2/uzk_cvproject/fly_cnn_old.py:69\u001b[0m, in \u001b[0;36mCNN_Fly.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Encode\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(enc)\n\u001b[1;32m     71\u001b[0m     z_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(flat)\n",
      "File \u001b[0;32m/scratch/cv-course2025/group2/uzk_cvproject/fly_cnn_old.py:46\u001b[0m, in \u001b[0;36mCNN_Fly.encoder\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2(x))\n\u001b[1;32m     48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me3(x))\n",
      "File \u001b[0;32m~/assignments/ae-vae/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/assignments/ae-vae/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/assignments/ae-vae/lib/python3.9/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/assignments/ae-vae/lib/python3.9/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 3, 3], expected input[1, 3, 480, 960] to have 1 channels, but got 3 channels instead"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize model\n",
    "def visualize_predictions(model, dataset, device=\"cuda\", num_samples=5):\n",
    "    model.eval()\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 4))\n",
    "    axes = axes if num_samples > 1 else [axes]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        img, true_kp = dataset[i]\n",
    "        # Since the model expects a batch, we unsqueeze to create a \"batch\" \n",
    "        # consisting of a single image.\n",
    "        #input_img = img.unsqueeze(0).to(device)  # [1, 3, H, W]\n",
    "        img.to(device)\n",
    "        \n",
    "        # Permute to get from [C, H, W] to [H, W, 3]\n",
    "        vis_img = img.permute(1, 2, 0).cpu().numpy() \n",
    "\n",
    "        # Run model\n",
    "        with torch.no_grad():\n",
    "            # Model returns a batch, meaning [B, J, 2], with squeezing 0 we \n",
    "            # end up with [J, 2].\n",
    "            pred_kp, _, _, _ = model(img).cpu() \n",
    "\n",
    "        # Get H, W from the visualize image shape. We permuted earlier, \n",
    "        # meaning now we just cut of the last dim to end up with [H, W].\n",
    "        H, W = vis_img.shape[:2]\n",
    "        pred_px = pred_kp.clone()\n",
    "        pred_px[:, 0] *= H\n",
    "        pred_px[:, 1] *= W\n",
    "\n",
    "        true_px = true_kp.clone()\n",
    "        true_px[:, 0] *= H\n",
    "        true_px[:, 1] *= W\n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.imshow(vis_img)\n",
    "        valid_pred = (\n",
    "            (pred_px[:, 0] >= 0) & (pred_px[:, 0] < H) &\n",
    "            (pred_px[:, 1] >= 0) & (pred_px[:, 1] < W)\n",
    "        )\n",
    "        pred_px = pred_px[valid_pred]\n",
    "        valid_true = (\n",
    "            (true_px[:, 0] >= 0) & (true_px[:, 0] < H) &\n",
    "            (true_px[:, 1] >= 0) & (true_px[:, 1] < W)\n",
    "        )\n",
    "        true_px = true_px[valid_true]\n",
    "        ax.scatter(pred_px[:, 1], pred_px[:, 0], c='r', label='Predicted', s=10)\n",
    "        ax.scatter(true_px[:, 1], true_px[:, 0], c='g', label='GT', s=10, alpha=0.6)\n",
    "        ax.set_title(f\"Sample {i}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    axes[0].legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Pre-Trained ResNet50 Model (ImageNet) to have a better starting point \n",
    "model = CNN_Fly((480, 960), 32)\n",
    "test_dataset = FLYDataset(\"/scratch/cv-course2025/group2/data\", mode=\"test\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Use CPU for debug so you do not use too much GPU time\n",
    "print(f\"Used Device: {device}\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"/scratch/cv-course2025/group2/uzk_cvproject/fly-test-resnet50.pt\"))\n",
    "model.to(device)\n",
    "visualize_predictions(model, test_dataset, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
